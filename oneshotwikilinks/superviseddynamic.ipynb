{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b84a1d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b279b8",
   "metadata": {
    "code_folding": [
     2,
     31,
     45,
     63,
     81,
     109,
     126
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EasyAcc:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.sum = 0\n",
    "        self.sumsq = 0\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        self.n += 1\n",
    "        self.sum += other\n",
    "        self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        self.n += 1\n",
    "        self.sum -= other\n",
    "        self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / max(self.n, 1)\n",
    "\n",
    "    def var(self):\n",
    "        from math import sqrt\n",
    "        return sqrt(self.sumsq / max(self.n, 1) - self.mean()**2)\n",
    "\n",
    "    def semean(self):\n",
    "        from math import sqrt\n",
    "        return self.var() / sqrt(max(self.n, 1))\n",
    "\n",
    "def categoryCount():\n",
    "    from collections import defaultdict\n",
    "    import gzip\n",
    "    import json\n",
    "        \n",
    "    counts = {}\n",
    "\n",
    "    with gzip.open('entityfreq.gz', 'rt') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                freq, entity = line.strip().split()\n",
    "            except:\n",
    "                continue\n",
    "            counts[entity] = int(freq)\n",
    "            \n",
    "    return counts\n",
    "\n",
    "def getCategories(threshold):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import gzip\n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        \n",
    "    for entity, freq in categoryCount().items():\n",
    "        if freq >= threshold:\n",
    "            niceentity = re.sub(r'_', r' ', entity)\n",
    "            embedcat = model.encode([niceentity])[0]\n",
    "            yield entity, embedcat\n",
    "\n",
    "def datasetStats(threshold):\n",
    "    numclasses = len([ entity for entity, freq in categoryCount().items() if freq >= threshold ])\n",
    "    return { 'numclasses': numclasses, 'numexamples': threshold * numclasses }\n",
    "            \n",
    "def makeData(threshold, categories):\n",
    "    from collections import defaultdict\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import json\n",
    "    \n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    catcount = defaultdict(int)\n",
    "    \n",
    "    with open('shuffled_dedup_entities.tsv') as f:\n",
    "        batchline, batchencode, batchentity = [], [], []\n",
    "        for line in f:\n",
    "            try:\n",
    "                entity, pre, mention, post = line.strip().split('\\t')\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            if entity in categories and catcount[entity] < threshold:\n",
    "                catcount[entity] += 1\n",
    "                batchline.append(line)\n",
    "                batchencode.append(pre)\n",
    "                batchencode.append(post)\n",
    "                batchentity.append(entity)\n",
    "\n",
    "                if len(batchline) == 5:\n",
    "                    embed = model.encode(batchencode)\n",
    "\n",
    "                    for n, (line, entity) in enumerate(zip(batchline, batchentity)):\n",
    "                        embedpre, embedpost = embed[2*n], embed[2*n+1]\n",
    "                        entityord, entityvec = categories[entity]\n",
    "                        yield { 'line': line, \n",
    "                                'entityord': entityord, \n",
    "                                'entityvec': entityvec,\n",
    "                                'pre': embedpre, \n",
    "                                'post': embedpost }\n",
    "\n",
    "                    batchline, batchencode, batchentity = [], [], []\n",
    "                \n",
    "        if len(batchline):\n",
    "            embed = model.encode(batchencode)\n",
    "\n",
    "            for n, (line, entity) in enumerate(zip(batchline, batchentity)):\n",
    "                embedpre, embedpost = embed[2*n], embed[2*n+1]\n",
    "                entityord, entityvec = categories[entity]\n",
    "                yield { 'line': line, \n",
    "                        'entityord': entityord, \n",
    "                        'entityvec': entityvec,\n",
    "                        'pre': embedpre, \n",
    "                        'post': embedpost }\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, threshold):\n",
    "        from tqdm.notebook import tqdm\n",
    "        self.labelfeats = { k: (n, v) for n, (k, v) in enumerate(getCategories(threshold)) } \n",
    "        Xs = []\n",
    "        ys = []\n",
    "        for n, what in tqdm(enumerate(makeData(threshold, self.labelfeats))):\n",
    "#             if n >= 1000:\n",
    "#                 break\n",
    "            pre = torch.tensor(what['pre'])\n",
    "            post = torch.tensor(what['post'])\n",
    "            Xs.append(torch.cat((pre, post)).unsqueeze(0))\n",
    "            ys.append(what['entityord'])\n",
    "\n",
    "        self.Xs = torch.cat(Xs, dim=0)\n",
    "        self.ys = torch.LongTensor(ys)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.Xs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        return self.Xs[index], self.ys[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b1d7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'numclasses': 311, 'numexamples': 622000},\n",
       " {'numclasses': 1154, 'numexamples': 1154000},\n",
       " {'numclasses': 14031, 'numexamples': 2806200})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetStats(2000), datasetStats(1000), datasetStats(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525057b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## This takes time, run once only (days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f34662",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea60f54206940ea94d9ba331dd3e0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def makeMyDataset(threshold):\n",
    "    import gzip\n",
    "    \n",
    "    foo = MyDataset(threshold)\n",
    "    with gzip.open(f'mydataset.{threshold}.pickle.gz', 'wb') as handle:\n",
    "        import pickle\n",
    "        pickle.dump(foo, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "makeMyDataset(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2c019",
   "metadata": {},
   "source": [
    "## Load cached processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f8f13c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loadMyDataset(threshold):\n",
    "    import gzip\n",
    "    \n",
    "    with gzip.open(f'mydataset.{threshold}.pickle.gz', 'rb') as handle:\n",
    "        import pickle\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbacba3",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'best_constant_answer': 'public_domain',\n",
       "  'best_constant_average_logloss': 5.739792823791504,\n",
       "  'best_constant_average_accuracy': 0.003215434083601286},\n",
       " {'best_constant_answer': 'public_domain',\n",
       "  'best_constant_average_logloss': 7.050989627838135,\n",
       "  'best_constant_average_accuracy': 0.0008665511265164644},\n",
       " {'best_constant_answer': 'weight_gain',\n",
       "  'best_constant_average_logloss': 9.54902458190918,\n",
       "  'best_constant_average_accuracy': 7.127075760815338e-05})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best constant predictor\n",
    "# if you don't beat this, you have a problem\n",
    "\n",
    "def bestconstant(threshold):\n",
    "    from math import fsum\n",
    "    \n",
    "    counts = { k: threshold for k, v in categoryCount().items() if v >= threshold }\n",
    "    sumcounts = fsum(v for v in counts.values())\n",
    "    predict = torch.Tensor([ v / sumcounts for v in counts.values() ]).unsqueeze(0)\n",
    "    log_loss = torch.nn.CrossEntropyLoss()\n",
    "    sumloss, denom = EasyAcc(), 0\n",
    "    \n",
    "    for m, k in enumerate(counts.keys()):\n",
    "        n = counts[k]\n",
    "        actual = torch.LongTensor([m])\n",
    "        sumloss += n * log_loss(predict, actual).item()\n",
    "        denom += n\n",
    "    \n",
    "    return { 'best_constant_answer': max((v, k) for k, v in counts.items())[1], \n",
    "             'best_constant_average_logloss': sumloss.sum / denom,\n",
    "             'best_constant_average_accuracy': max(v for v in counts.values()) / denom }            \n",
    "\n",
    "bestconstant(2000), bestconstant(1000), bestconstant(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f6a4ca2",
   "metadata": {
    "code_folding": [
     0,
     35
    ]
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, d, device):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.zeros(d, d, device=device))\n",
    "        self.afunc = torch.nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X + 0.001 * self.afunc(torch.matmul(X, self.W))\n",
    "    \n",
    "class BilinearResidual(torch.nn.Module):\n",
    "    def __init__(self, dobs, daction, device, depth):\n",
    "        super(BilinearResidual, self).__init__()\n",
    "        \n",
    "        self.block = torch.nn.Sequential(*[ResidualBlock(dobs, device) for _ in range(depth) ])\n",
    "        self.W = torch.nn.Parameter(torch.zeros(dobs, daction, device=device))\n",
    "\n",
    "    def forward(self, Xs, Zs):\n",
    "        return torch.matmul(torch.matmul(self.block(Xs), self.W), Zs.T)\n",
    "\n",
    "def learnOnline(dataset, initlr, tzero, rank, depth, cuda=False, seed=4545):\n",
    "    import time\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    labelfeatsdict = { n: v for n, v in dataset.labelfeats.values() }\n",
    "    labelfeats = [ torch.tensor(labelfeatsdict[n]).float().unsqueeze(0) for n in range(len(labelfeatsdict)) ]\n",
    "    Zs = torch.cat(labelfeats, dim=0)\n",
    "    \n",
    "    if cuda:\n",
    "        Zs = Zs.cuda()\n",
    "    \n",
    "    if rank is not None:\n",
    "        with torch.no_grad():\n",
    "            U, S, Vh = torch.linalg.svd(Zs, full_matrices=False)\n",
    "            Zs = U[:, :rank] @ torch.diag(S[:rank])\n",
    "        \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    model = None\n",
    "    log_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    print('{:<5s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}'.format('n', 'loss', 'since last', 'acc', 'since last', 'dt (sec)'), flush=True)\n",
    "    avloss, acc, sincelast, accsincelast = EasyAcc(), EasyAcc(), EasyAcc(), EasyAcc()\n",
    "    \n",
    "    for bno, (Xs, ys) in enumerate(generator):\n",
    "        Xs, ys = Xs.to(Zs.device), ys.to(Zs.device)\n",
    "        \n",
    "        if model is None:\n",
    "            import numpy as np\n",
    "            model = BilinearResidual(dobs=Xs.shape[1], daction=Zs.shape[1], depth=depth, device=Zs.device)\n",
    "            opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: np.sqrt(tzero) / np.sqrt(tzero + t))\n",
    "            start = time.time()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        score = model.forward(0.0001 * Xs, Zs)\n",
    "        loss = log_loss(score, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(score, dim=1)\n",
    "            acc += torch.mean((pred == ys).float())\n",
    "            accsincelast += torch.mean((pred == ys).float())\n",
    "            avloss += loss\n",
    "            sincelast += loss\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            print('{:<5d}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}'.format(avloss.n, avloss.mean(), sincelast.mean(), \n",
    "                                                                                         acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "                  flush=True)\n",
    "            sincelast, accsincelast = EasyAcc(), EasyAcc()\n",
    "\n",
    "    print('{:<5d}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}'.format(avloss.n, avloss.mean(), sincelast.mean(), \n",
    "                                                                                 acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b396d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = loadMyDataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e947df90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss      \tsince last\tacc       \tsince last\tdt (sec)  \n",
      "1    \t5.73979   \t5.73979   \t0.03125   \t0.03125   \t1.59583   \n",
      "2    \t5.74189   \t5.74398   \t0.04688   \t0.06250   \t1.61233   \n",
      "3    \t5.74583   \t5.75372   \t0.03125   \t0.00000   \t1.62748   \n",
      "5    \t5.63384   \t5.46584   \t0.01875   \t0.00000   \t1.65971   \n",
      "9    \t5.49162   \t5.31384   \t0.04167   \t0.07031   \t1.71931   \n",
      "17   \t5.37390   \t5.24147   \t0.03676   \t0.03125   \t1.83441   \n",
      "33   \t5.09394   \t4.79647   \t0.05019   \t0.06445   \t2.07867   \n",
      "65   \t4.78399   \t4.46436   \t0.06923   \t0.08887   \t2.52995   \n",
      "129  \t4.49957   \t4.21070   \t0.09859   \t0.12842   \t3.39347   \n",
      "257  \t4.25538   \t4.00928   \t0.12707   \t0.15576   \t5.09717   \n",
      "513  \t4.04204   \t3.82787   \t0.15515   \t0.18335   \t8.51150   \n",
      "1025 \t3.83618   \t3.62993   \t0.18399   \t0.21289   \t15.28262  \n",
      "2049 \t3.67509   \t3.51382   \t0.20579   \t0.22760   \t29.04722  \n",
      "4097 \t3.53127   \t3.38738   \t0.22584   \t0.24590   \t56.78693  \n",
      "8193 \t3.39453   \t3.25775   \t0.24602   \t0.26620   \t111.52929 \n",
      "16385\t3.25441   \t3.11428   \t0.26725   \t0.28848   \t223.34430 \n",
      "19438\t3.22275   \t3.05279   \t0.27280   \t0.30258   \t266.35592 \n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, initlr=1.6, tzero=1000, rank=50, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee71a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss      \tsince last\tacc       \tsince last\tdt (sec)  \n",
      "1    \t5.73979   \t5.73979   \t0.03125   \t0.03125   \t0.02975   \n",
      "2    \t5.74189   \t5.74398   \t0.04688   \t0.06250   \t0.06424   \n",
      "3    \t5.73980   \t5.73563   \t0.03125   \t0.00000   \t0.09255   \n",
      "5    \t5.63467   \t5.47698   \t0.01875   \t0.00000   \t0.15084   \n",
      "9    \t5.48668   \t5.30170   \t0.04167   \t0.07031   \t0.27073   \n",
      "17   \t5.35737   \t5.21189   \t0.03493   \t0.02734   \t0.50968   \n",
      "33   \t5.09352   \t4.81317   \t0.05019   \t0.06641   \t0.94030   \n",
      "65   \t4.80017   \t4.49766   \t0.06827   \t0.08691   \t1.80242   \n",
      "129  \t4.52182   \t4.23912   \t0.09593   \t0.12402   \t3.57374   \n",
      "257  \t4.27489   \t4.02603   \t0.12622   \t0.15674   \t7.00461   \n",
      "513  \t4.05326   \t3.83076   \t0.15363   \t0.18115   \t14.02894  \n",
      "1025 \t3.84507   \t3.63647   \t0.18229   \t0.21100   \t28.35510  \n",
      "2049 \t3.67987   \t3.51450   \t0.20480   \t0.22733   \t56.35309  \n",
      "4097 \t3.52884   \t3.37773   \t0.22626   \t0.24774   \t112.48561 \n",
      "8193 \t3.38435   \t3.23981   \t0.24758   \t0.26890   \t225.65657 \n",
      "16385\t3.23519   \t3.08601   \t0.27069   \t0.29380   \t450.13423 \n",
      "19438\t3.20075   \t3.01592   \t0.27683   \t0.30979   \t535.03856 \n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, initlr=1.6, tzero=1000, rank=50, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67beb4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss      \tsince last\tacc       \tsince last\tdt (sec)  \n",
      "1    \t5.73979   \t5.73979   \t0.03125   \t0.03125   \t0.04361   \n",
      "2    \t5.74189   \t5.74398   \t0.04688   \t0.06250   \t0.08212   \n",
      "3    \t5.73457   \t5.71995   \t0.03125   \t0.00000   \t0.12103   \n",
      "5    \t5.64684   \t5.51525   \t0.02500   \t0.01562   \t0.19858   \n",
      "9    \t5.49352   \t5.30186   \t0.04861   \t0.07812   \t0.36066   \n",
      "17   \t5.36423   \t5.21878   \t0.03676   \t0.02344   \t0.69372   \n",
      "33   \t5.10200   \t4.82338   \t0.05019   \t0.06445   \t1.30164   \n",
      "65   \t4.81347   \t4.51592   \t0.07019   \t0.09082   \t2.50194   \n",
      "129  \t4.54179   \t4.26587   \t0.09254   \t0.11523   \t4.96825   \n",
      "257  \t4.30416   \t4.06468   \t0.12281   \t0.15332   \t9.97540   \n",
      "513  \t4.08976   \t3.87452   \t0.15046   \t0.17822   \t19.43409  \n",
      "1025 \t3.87696   \t3.66374   \t0.17930   \t0.20819   \t38.36506  \n",
      "2049 \t3.71016   \t3.54320   \t0.20147   \t0.22366   \t76.37812  \n",
      "4097 \t3.55729   \t3.40435   \t0.22384   \t0.24622   \t151.93329 \n",
      "8193 \t3.40762   \t3.25791   \t0.24648   \t0.26913   \t305.66595 \n",
      "16385\t3.25106   \t3.09444   \t0.27061   \t0.29475   \t606.47451 \n",
      "19438\t3.21479   \t3.02009   \t0.27677   \t0.30984   \t716.28779 \n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, initlr=1.6, tzero=1000, rank=50, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c3b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = loadMyDataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15cb7e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss    \tsince last\tacc     \tacc since last\n",
      "1    \t7.05099 \t7.05099 \t0.00000 \t0.00000 \n",
      "2    \t7.03190 \t7.01281 \t0.00000 \t0.00000 \n",
      "3    \t7.04831 \t7.08113 \t0.00000 \t0.00000 \n",
      "5    \t7.04976 \t7.05195 \t0.00000 \t0.00000 \n",
      "9    \t6.96595 \t6.86118 \t0.00347 \t0.00781 \n",
      "17   \t6.87694 \t6.77679 \t0.00735 \t0.01172 \n",
      "33   \t6.70233 \t6.51681 \t0.01042 \t0.01367 \n",
      "65   \t6.45273 \t6.19534 \t0.02067 \t0.03125 \n",
      "129  \t6.11979 \t5.78165 \t0.03464 \t0.04883 \n",
      "257  \t5.81036 \t5.49852 \t0.04815 \t0.06177 \n",
      "513  \t5.51839 \t5.22528 \t0.06335 \t0.07861 \n",
      "1025 \t5.28478 \t5.05070 \t0.07966 \t0.09601 \n",
      "2049 \t5.10783 \t4.93069 \t0.09255 \t0.10544 \n",
      "4097 \t4.96485 \t4.82180 \t0.10396 \t0.11537 \n",
      "8193 \t4.85280 \t4.74070 \t0.11337 \t0.12280 \n",
      "16385\t4.76203 \t4.67124 \t0.12150 \t0.12963 \n",
      "32769\t4.69070 \t4.61934 \t0.12856 \t0.13561 \n",
      "36063\t4.68256 \t4.60159 \t0.12936 \t0.13735 \n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, initlr=8e-1, tzero=400, rank=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c06286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
