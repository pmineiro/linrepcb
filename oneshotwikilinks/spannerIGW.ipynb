{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b84a1d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b279b8",
   "metadata": {
    "code_folding": [
     2,
     31,
     45,
     63,
     81,
     109,
     126
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EasyAcc:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.sum = 0\n",
    "        self.sumsq = 0\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        self.n += 1\n",
    "        self.sum += other\n",
    "        self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        self.n += 1\n",
    "        self.sum -= other\n",
    "        self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / max(self.n, 1)\n",
    "\n",
    "    def var(self):\n",
    "        from math import sqrt\n",
    "        return sqrt(self.sumsq / max(self.n, 1) - self.mean()**2)\n",
    "\n",
    "    def semean(self):\n",
    "        from math import sqrt\n",
    "        return self.var() / sqrt(max(self.n, 1))\n",
    "\n",
    "def categoryCount():\n",
    "    from collections import defaultdict\n",
    "    import gzip\n",
    "    import json\n",
    "        \n",
    "    counts = {}\n",
    "\n",
    "    with gzip.open('entityfreq.gz', 'rt') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                freq, entity = line.strip().split()\n",
    "            except:\n",
    "                continue\n",
    "            counts[entity] = int(freq)\n",
    "            \n",
    "    return counts\n",
    "\n",
    "def getCategories(threshold):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import gzip\n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        \n",
    "    for entity, freq in categoryCount().items():\n",
    "        if freq >= threshold:\n",
    "            niceentity = re.sub(r'_', r' ', entity)\n",
    "            embedcat = model.encode([niceentity])[0]\n",
    "            yield entity, embedcat\n",
    "\n",
    "def datasetStats(threshold):\n",
    "    numclasses = len([ entity for entity, freq in categoryCount().items() if freq >= threshold ])\n",
    "    return { 'numclasses': numclasses, 'numexamples': threshold * numclasses }\n",
    "            \n",
    "def makeData(threshold, categories):\n",
    "    from collections import defaultdict\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import json\n",
    "    \n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    catcount = defaultdict(int)\n",
    "    \n",
    "    with open('shuffled_dedup_entities.tsv') as f:\n",
    "        batchline, batchencode, batchentity = [], [], []\n",
    "        for line in f:\n",
    "            try:\n",
    "                entity, pre, mention, post = line.strip().split('\\t')\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            if entity in categories and catcount[entity] < threshold:\n",
    "                catcount[entity] += 1\n",
    "                batchline.append(line)\n",
    "                batchencode.append(pre)\n",
    "                batchencode.append(post)\n",
    "                batchentity.append(entity)\n",
    "\n",
    "                if len(batchline) == 5:\n",
    "                    embed = model.encode(batchencode)\n",
    "\n",
    "                    for n, (line, entity) in enumerate(zip(batchline, batchentity)):\n",
    "                        embedpre, embedpost = embed[2*n], embed[2*n+1]\n",
    "                        entityord, entityvec = categories[entity]\n",
    "                        yield { 'line': line, \n",
    "                                'entityord': entityord, \n",
    "                                'entityvec': entityvec,\n",
    "                                'pre': embedpre, \n",
    "                                'post': embedpost }\n",
    "\n",
    "                    batchline, batchencode, batchentity = [], [], []\n",
    "                \n",
    "        if len(batchline):\n",
    "            embed = model.encode(batchencode)\n",
    "\n",
    "            for n, (line, entity) in enumerate(zip(batchline, batchentity)):\n",
    "                embedpre, embedpost = embed[2*n], embed[2*n+1]\n",
    "                entityord, entityvec = categories[entity]\n",
    "                yield { 'line': line, \n",
    "                        'entityord': entityord, \n",
    "                        'entityvec': entityvec,\n",
    "                        'pre': embedpre, \n",
    "                        'post': embedpost }\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, threshold):\n",
    "        from tqdm.notebook import tqdm\n",
    "        self.labelfeats = { k: (n, v) for n, (k, v) in enumerate(getCategories(threshold)) } \n",
    "        Xs = []\n",
    "        ys = []\n",
    "        for n, what in tqdm(enumerate(makeData(threshold, self.labelfeats))):\n",
    "#             if n >= 1000:\n",
    "#                 break\n",
    "            pre = torch.tensor(what['pre'])\n",
    "            post = torch.tensor(what['post'])\n",
    "            Xs.append(torch.cat((pre, post)).unsqueeze(0))\n",
    "            ys.append(what['entityord'])\n",
    "\n",
    "        self.Xs = torch.cat(Xs, dim=0)\n",
    "        self.ys = torch.LongTensor(ys)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.Xs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        return self.Xs[index], self.ys[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b1d7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'numclasses': 311, 'numexamples': 622000},\n",
       " {'numclasses': 1154, 'numexamples': 1154000},\n",
       " {'numclasses': 32089, 'numexamples': 3208900})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetStats(2000), datasetStats(1000), datasetStats(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb269bf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## This takes time, run once only (days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f34662",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea60f54206940ea94d9ba331dd3e0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def makeMyDataset(threshold):\n",
    "    import gzip\n",
    "    \n",
    "    foo = MyDataset(threshold)\n",
    "    with gzip.open(f'mydataset.{threshold}.pickle.gz', 'wb') as handle:\n",
    "        import pickle\n",
    "        pickle.dump(foo, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "makeMyDataset(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2c019",
   "metadata": {},
   "source": [
    "## Load cached processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f8f13c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loadMyDataset(threshold):\n",
    "    import gzip\n",
    "    \n",
    "    with gzip.open(f'mydataset.{threshold}.pickle.gz', 'rb') as handle:\n",
    "        import pickle\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbacba3",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'best_constant_answer': 'public_domain',\n",
       "  'best_constant_average_logloss': 5.739792823791504,\n",
       "  'best_constant_average_accuracy': 0.003215434083601286},\n",
       " {'best_constant_answer': 'public_domain',\n",
       "  'best_constant_average_logloss': 7.050989627838135,\n",
       "  'best_constant_average_accuracy': 0.0008665511265164644},\n",
       " {'best_constant_answer': 'weight_gain',\n",
       "  'best_constant_average_logloss': 9.54902458190918,\n",
       "  'best_constant_average_accuracy': 7.127075760815338e-05})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best constant predictor\n",
    "# if you don't beat this, you have a problem\n",
    "\n",
    "def bestconstant(threshold):\n",
    "    from math import fsum\n",
    "    \n",
    "    counts = { k: threshold for k, v in categoryCount().items() if v >= threshold }\n",
    "    sumcounts = fsum(v for v in counts.values())\n",
    "    predict = torch.Tensor([ v / sumcounts for v in counts.values() ]).unsqueeze(0)\n",
    "    log_loss = torch.nn.CrossEntropyLoss()\n",
    "    sumloss, denom = EasyAcc(), 0\n",
    "    \n",
    "    for m, k in enumerate(counts.keys()):\n",
    "        n = counts[k]\n",
    "        actual = torch.LongTensor([m])\n",
    "        sumloss += n * log_loss(predict, actual).item()\n",
    "        denom += n\n",
    "    \n",
    "    return { 'best_constant_answer': max((v, k) for k, v in counts.items())[1], \n",
    "             'best_constant_average_logloss': sumloss.sum / denom,\n",
    "             'best_constant_average_accuracy': max(v for v in counts.values()) / denom }            \n",
    "\n",
    "bestconstant(2000), bestconstant(1000), bestconstant(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c360ad75",
   "metadata": {
    "code_folding": [
     0,
     35
    ]
   },
   "outputs": [],
   "source": [
    "class Bilinear(torch.nn.Module):\n",
    "    def __init__(self, dobs, daction, naction, device):\n",
    "        super(Bilinear, self).__init__()\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.zeros(dobs, daction, device=device))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, device=device))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, Xs, Zs):\n",
    "        return torch.matmul(torch.matmul(Xs, self.W), Zs.T) + self.b\n",
    "        \n",
    "    def preq1(self, logits):\n",
    "        return self.sigmoid(logits)\n",
    "    \n",
    "class RankOneDetset(object):\n",
    "    def __init__(self, batch_size, rawactions):\n",
    "        self.N = batch_size\n",
    "        self.rawactions = rawactions\n",
    "        self.K, self.D = rawactions.shape\n",
    "        self.device = rawactions.device\n",
    "        \n",
    "        self.batcheye = torch.eye(self.D, device=self.device).unsqueeze(0).expand(self.N, -1, -1)\n",
    "        self.S = self.batcheye.clone()\n",
    "        self.Sinv = self.batcheye.clone()\n",
    "        self.logdetfac = torch.zeros(self.N, 1, device=self.device)\n",
    "        \n",
    "    def computePhi(self, i): \n",
    "        # Sprime_a <- replace column i of S with action a where det(S)=1\n",
    "        # Sprime_a = S + (a - S_i) e_i^\\top = S + u v^\\top\n",
    "        # det(Sprime_a) = det(S) (1 + e_i^\\top S^{-1} (a - S_i))\n",
    "        #               = (1 - (S^{-T} e_i)^\\top S_i) + (S^{-T} e_i)^\\top a\n",
    "        #               = 0 + \\phi^\\top a\n",
    "        \n",
    "        #Sinvtopei = torch.linalg.solve(torch.transpose(self.S, 1, 2), self.batcheye[:,:,i])\n",
    "        Sinvtopei = self.Sinv[:, i, :]\n",
    "        return Sinvtopei, self.logdetfac\n",
    "    \n",
    "    def computeAllPhi(self):\n",
    "        return self.Sinv, self.logdetfac\n",
    "    \n",
    "    def updateAll(self, colstar, fstar, astar, denom):\n",
    "        Y = torch.gather(input=self.rawactions.unsqueeze(0).expand(self.N, -1, -1), \n",
    "                         dim=1, \n",
    "                         index=astar.reshape(self.N, 1, 1).expand(self.N, 1, self.D)\n",
    "                        ).squeeze(1)\n",
    "        Ydenom = torch.gather(input=denom, dim=1, index=astar)\n",
    "        Y /= Ydenom\n",
    "        Y /= torch.exp(self.logdetfac).reshape(self.N, 1)\n",
    "        \n",
    "        u = Y - torch.gather(input=self.S, dim=2, index=colstar.unsqueeze(1).expand(self.N, self.D, 1)).squeeze(2)\n",
    "        Sinvu = torch.bmm(self.Sinv, u.unsqueeze(2)).squeeze(2)\n",
    "        vtopSinv = torch.gather(input=self.Sinv, dim=1, index=colstar.unsqueeze(1).expand(self.N, 1, self.D)).squeeze(1)\n",
    "        vtopSinvu = torch.gather(input=Sinvu, dim=1, index=colstar).unsqueeze(2)\n",
    "        self.Sinv -= (1 / (1 + vtopSinvu)) * torch.bmm(Sinvu.unsqueeze(2), vtopSinv.unsqueeze(1))\n",
    "\n",
    "        self.S.scatter_(index=colstar.unsqueeze(1).expand(self.N, self.D, 1), \n",
    "                        dim=2, \n",
    "                        src=Y.unsqueeze(2))\n",
    "        thislogdet = 1/self.D * (torch.log(fstar) - self.logdetfac)\n",
    "        scale = torch.exp(thislogdet).reshape(self.N, 1, 1)\n",
    "        self.S /= scale\n",
    "        self.Sinv *= scale\n",
    "        self.logdetfac += thislogdet\n",
    "    \n",
    "    def updateCoord(self, i, fstar, astar, denom):\n",
    "        Y = torch.gather(input=self.rawactions.unsqueeze(0).expand(self.N, -1, -1), \n",
    "                         dim=1, \n",
    "                         index=astar.reshape(self.N, 1, 1).expand(self.N, 1, self.D)\n",
    "                        ).squeeze(1)\n",
    "        Ydenom = torch.gather(input=denom, dim=1, index=astar)\n",
    "        Y /= Ydenom\n",
    "        Y /= torch.exp(self.logdetfac).reshape(self.N, 1)\n",
    "\n",
    "        # replace column i of S with y\n",
    "        # -----------------------------\n",
    "        # Sprime = S + (y - S_i) e_i^\\top = S + u v^\\top\n",
    "        # Sprime^{-1} = S^{-1} - 1/(1 + v^\\top S^{-1} u) (S^{-1} u) (v^\\top S^{-1})^\\top\n",
    "        \n",
    "        u = Y - self.S[:, :, i]\n",
    "        Sinvu = torch.bmm(self.Sinv, u.unsqueeze(2)).squeeze(2)\n",
    "        vtopSinv = self.Sinv[:, i, :]\n",
    "        vtopSinvu = Sinvu[:, i].unsqueeze(1).unsqueeze(2)\n",
    "        self.Sinv -= (1 / (1 + vtopSinvu)) * torch.bmm(Sinvu.unsqueeze(2), vtopSinv.unsqueeze(1))\n",
    "        \n",
    "        self.S[:,:,i] = Y\n",
    "        thislogdet = 1/self.D * (torch.log(fstar) - self.logdetfac)\n",
    "        scale = torch.exp(thislogdet).reshape(self.N, 1, 1)\n",
    "        self.S /= scale\n",
    "        self.Sinv *= scale\n",
    "        self.logdetfac += thislogdet\n",
    "\n",
    "class SpannerIGW(torch.nn.Module):\n",
    "    def __init__(self, actions, iota):\n",
    "        super(SpannerIGW, self).__init__()\n",
    "        \n",
    "        self.rawactions = actions\n",
    "        self.iota = iota\n",
    "          \n",
    "    def _make_spanner(self, denom):\n",
    "        from math import log\n",
    "\n",
    "        # Algorithm 4 Approximate Barycentric Identification (Awerbuch and Kleinberg, 2008)\n",
    "        C = 2\n",
    "        \n",
    "        N, _ = denom.shape\n",
    "        K, D = self.rawactions.shape\n",
    "        device = self.rawactions.device\n",
    "        detset = RankOneDetset(N, self.rawactions)\n",
    "        design = torch.zeros(N, D, device=device).long()\n",
    "                \n",
    "        for i in range(D):\n",
    "            psi, _ = detset.computePhi(i)\n",
    "            dets = torch.abs(torch.matmul(self.rawactions, psi.T)).T / denom \n",
    "            fstar, astar = torch.max(dets, dim=1, keepdim=True)\n",
    "            design[:, i] = astar.squeeze(1)\n",
    "            detset.updateCoord(i, fstar, astar, denom)\n",
    "                        \n",
    "        for _ in range(int(D * log(D))):\n",
    "            allpsi, logdetfac = detset.computeAllPhi()\n",
    "            Z = self.rawactions.T.unsqueeze(0).expand(N, -1, -1)\n",
    "            dets = torch.abs(torch.bmm(allpsi, Z)) / denom.unsqueeze(1)\n",
    "            fstarcolumn, astarcolumn = torch.max(dets, dim=2)\n",
    "            fstar, colstar = torch.max(fstarcolumn, dim=1, keepdim=True)\n",
    "            astar = torch.gather(input=astarcolumn, dim=1, index=colstar)\n",
    "                                \n",
    "            if torch.any(fstar >= C * torch.exp(logdetfac)):\n",
    "                design.scatter_(index=colstar, dim=1, src=astar)\n",
    "                detset.updateAll(colstar, fstar, astar, denom)\n",
    "            else:\n",
    "                break\n",
    "              \n",
    "        return design\n",
    "\n",
    "    def _compute_denom(self, fhat, fhatstar):\n",
    "        N = fhat.shape[0]\n",
    "        d = self.rawactions.shape[1]\n",
    "        return torch.sqrt(1 + d + self.iota * (fhatstar - fhat))\n",
    "\n",
    "    def sample(self, fhat):\n",
    "        device = self.rawactions.device\n",
    "\n",
    "        N = fhat.shape[0]\n",
    "        D = self.rawactions.shape[1]\n",
    "        fhatstar, exploit = torch.max(fhat, dim=1, keepdim=True)\n",
    "        denom = self._compute_denom(fhat, fhatstar)\n",
    "        design = self._make_spanner(denom)\n",
    "        \n",
    "        exploreindex = torch.randint(high=D, size=(N, 1), device=fhat.device)\n",
    "        explore = torch.gather(input=design, dim=1, index=exploreindex)\n",
    "        fhatexplore = torch.gather(input=fhat, dim=1, index=explore) \n",
    "        probs = D / (1 + D + self.iota * (fhatstar - fhatexplore))\n",
    "        unif = torch.rand(size=(N, 1), device=fhat.device)\n",
    "        shouldexplore = (unif <= probs).long()\n",
    "        return (exploit + shouldexplore * (explore - exploit)).squeeze(1)\n",
    "\n",
    "def learnOnline(dataset, seed=4545, rank=None, initlr=4e-1, tzero=100000, iota=1000, batch_size=32, cuda=False, extra=0):\n",
    "    import time\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    labelfeatsdict = { n: v for n, v in dataset.labelfeats.values() }\n",
    "    labelfeats = [ torch.tensor(labelfeatsdict[n]).float().unsqueeze(0) for n in range(len(labelfeatsdict)) ]\n",
    "    Zs = torch.cat(labelfeats, dim=0)\n",
    "    \n",
    "    if cuda:\n",
    "        Zs = Zs.cuda()\n",
    "    \n",
    "    if rank is not None:\n",
    "        with torch.no_grad():\n",
    "            U, S, Vh = torch.linalg.svd(Zs, full_matrices=False)\n",
    "            Zs = U[:, :rank] @ torch.diag(S[:rank])\n",
    "            \n",
    "    original = Zs.shape[0]\n",
    "    if extra > 0:\n",
    "        moarrows = Zs[-1,:].expand(extra, -1)\n",
    "        Zs = torch.cat((Zs, moarrows), dim=0)\n",
    "    \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = None\n",
    "    log_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    print('{:<5s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}'.format(\n",
    "            'n', 'loss', 'since last', 'acc', 'since last', 'reward', 'since last', 'dt (sec)'), \n",
    "          flush=True)\n",
    "    avloss, sincelast, acc, accsincelast, avreward, rewardsincelast = [ EasyAcc() for _ in range(6) ]\n",
    "    \n",
    "    for bno, (Xs, ys) in enumerate(generator):\n",
    "        Xs, ys = Xs.to(Zs.device), ys.to(Zs.device)\n",
    "                        \n",
    "        if model is None:\n",
    "            import numpy as np\n",
    "            model = Bilinear(dobs=Xs.shape[1], daction=Zs.shape[1], naction=Zs.shape[0], device=Zs.device)\n",
    "            sampler = SpannerIGW(actions=Zs, iota=iota)\n",
    "            opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: np.sqrt(tzero) / np.sqrt(tzero + t))\n",
    "            start = time.time()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            fhat = model.preq1(model.forward(0.0001 * Xs, Zs))\n",
    "            presample = sampler.sample(fhat)\n",
    "            sample = (presample >= original).long() * (original - 1 - presample) + presample\n",
    "            reward = (sample == ys).unsqueeze(1).float()\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        logit = model.forward(0.0001 * Xs, Zs)\n",
    "        samplelogit = torch.gather(input=logit, index=sample.unsqueeze(1), dim=1)\n",
    "        loss = log_loss(samplelogit, reward)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prepred = torch.argmax(logit, dim=1)\n",
    "            pred = (prepred >= original).long() * (original - 1 - prepred) + prepred\n",
    "            acc += torch.mean((pred == ys).float())\n",
    "            accsincelast += torch.mean((pred == ys).float())\n",
    "            avloss += loss\n",
    "            sincelast += loss\n",
    "            avreward += torch.mean(reward)\n",
    "            rewardsincelast += torch.mean(reward)\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            now = time.time()\n",
    "            print('{:<5d}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}'.format(\n",
    "                    avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), \n",
    "                    accsincelast.mean(), avreward.mean(), rewardsincelast.mean(),\n",
    "                    now - start),\n",
    "                  flush=True)\n",
    "            sincelast, accsincelast, rewardsincelast = [ EasyAcc() for _ in range(3) ]\n",
    "\n",
    "    now = time.time()\n",
    "    print('{:<5d}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}'.format(\n",
    "            avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), \n",
    "            accsincelast.mean(), avreward.mean(), rewardsincelast.mean(),\n",
    "            now - start),\n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0372c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = loadMyDataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "61630aee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.69315   \t0.69315   \t0.03125   \t0.03125   \t0.00000   \t0.00000   \t0.03622   \n",
      "2    \t0.61319   \t0.53324   \t0.01562   \t0.00000   \t0.00000   \t0.00000   \t0.07019   \n",
      "3    \t0.53545   \t0.37998   \t0.01042   \t0.00000   \t0.00000   \t0.00000   \t0.10568   \n",
      "5    \t0.41075   \t0.22369   \t0.00625   \t0.00000   \t0.00000   \t0.00000   \t0.17272   \n",
      "9    \t0.26150   \t0.07493   \t0.00347   \t0.00000   \t0.00000   \t0.00000   \t0.30608   \n",
      "17   \t0.14523   \t0.01443   \t0.00184   \t0.00000   \t0.00000   \t0.00000   \t0.58442   \n",
      "33   \t0.09242   \t0.03630   \t0.00473   \t0.00781   \t0.00284   \t0.00586   \t1.23097   \n",
      "65   \t0.06370   \t0.03408   \t0.00577   \t0.00684   \t0.00385   \t0.00488   \t2.53459   \n",
      "129  \t0.05744   \t0.05109   \t0.00872   \t0.01172   \t0.00630   \t0.00879   \t4.78488   \n",
      "257  \t0.04618   \t0.03483   \t0.01021   \t0.01172   \t0.00657   \t0.00684   \t9.22068   \n",
      "513  \t0.04758   \t0.04899   \t0.01377   \t0.01733   \t0.00926   \t0.01196   \t18.48969  \n",
      "1025 \t0.06114   \t0.07474   \t0.02168   \t0.02960   \t0.01598   \t0.02271   \t36.45802  \n",
      "2049 \t0.09296   \t0.12481   \t0.04008   \t0.05850   \t0.03102   \t0.04608   \t73.29954  \n",
      "4097 \t0.15623   \t0.21952   \t0.07497   \t0.10988   \t0.06179   \t0.09258   \t146.93394 \n",
      "8193 \t0.22904   \t0.30188   \t0.11707   \t0.15918   \t0.10044   \t0.13910   \t293.27152 \n",
      "16385\t0.29009   \t0.35114   \t0.15690   \t0.19674   \t0.13809   \t0.17575   \t584.88444 \n",
      "19438\t0.30240   \t0.36847   \t0.16569   \t0.21286   \t0.14657   \t0.19207   \t694.34162 \n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, cuda=False, initlr=0.33, tzero=100000, rank=50, iota=14000*50/311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e05f3f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.69315   \t0.69315   \t0.03125   \t0.03125   \t0.00000   \t0.00000   \t0.04451   \n",
      "2    \t0.61319   \t0.53324   \t0.01562   \t0.00000   \t0.00000   \t0.00000   \t0.10104   \n",
      "3    \t0.53545   \t0.37998   \t0.01042   \t0.00000   \t0.00000   \t0.00000   \t0.14878   \n",
      "5    \t0.41075   \t0.22369   \t0.00625   \t0.00000   \t0.00000   \t0.00000   \t0.23764   \n",
      "9    \t0.26150   \t0.07493   \t0.00347   \t0.00000   \t0.00000   \t0.00000   \t0.42172   \n",
      "17   \t0.14523   \t0.01443   \t0.00184   \t0.00000   \t0.00000   \t0.00000   \t0.77712   \n",
      "33   \t0.09242   \t0.03630   \t0.00473   \t0.00781   \t0.00284   \t0.00586   \t1.42541   \n",
      "65   \t0.06370   \t0.03408   \t0.00577   \t0.00684   \t0.00385   \t0.00488   \t2.77199   \n",
      "129  \t0.05744   \t0.05109   \t0.00872   \t0.01172   \t0.00630   \t0.00879   \t5.48490   \n",
      "257  \t0.04618   \t0.03483   \t0.01021   \t0.01172   \t0.00657   \t0.00684   \t11.57694  \n",
      "513  \t0.04758   \t0.04899   \t0.01377   \t0.01733   \t0.00926   \t0.01196   \t23.83897  \n",
      "1025 \t0.06114   \t0.07474   \t0.02168   \t0.02960   \t0.01598   \t0.02271   \t48.17678  \n",
      "2049 \t0.09296   \t0.12481   \t0.04008   \t0.05850   \t0.03102   \t0.04608   \t97.18060  \n",
      "4097 \t0.15402   \t0.21510   \t0.07421   \t0.10835   \t0.06092   \t0.09084   \t195.04118 \n",
      "8193 \t0.22726   \t0.30052   \t0.11639   \t0.15858   \t0.09976   \t0.13861   \t391.98023 \n",
      "16385\t0.28792   \t0.34858   \t0.15606   \t0.19574   \t0.13705   \t0.17434   \t788.54041 \n",
      "19438\t0.30034   \t0.36705   \t0.16489   \t0.21230   \t0.14552   \t0.19099   \t936.05022 \n"
     ]
    }
   ],
   "source": [
    "# statistically equivalent with duplicated actions\n",
    "learnOnline(mydata, cuda=False, initlr=0.33, tzero=100000, rank=50, iota=14000*50/311, extra=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041942a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = loadMyDataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnOnline(mydata, initlr=4e-1, tzero=100000, rank=50, iota=666, batch_size=32, cuda=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
