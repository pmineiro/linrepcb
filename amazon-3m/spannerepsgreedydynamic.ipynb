{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2cf823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EasyAcc:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.sum = 0\n",
    "        self.sumsq = 0\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        self.n += 1\n",
    "        self.sum += other\n",
    "        self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        self.n += 1\n",
    "        self.sum -= other\n",
    "        self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / max(self.n, 1)\n",
    "\n",
    "    def var(self):\n",
    "        from math import sqrt\n",
    "        return sqrt(self.sumsq / max(self.n, 1) - self.mean()**2)\n",
    "\n",
    "    def semean(self):\n",
    "        from math import sqrt\n",
    "        return self.var() / sqrt(max(self.n, 1))\n",
    "    \n",
    "# {'uid': '0000031909', \n",
    "#  'title': 'Girls Ballet Tutu Neon Pink\\n', \n",
    "#  'content': 'High quality 3 layer ballet tutu. 12 inches in length', \n",
    "#  'target_ind': [0, 1, 192406, 1327309, 1371116, 1371888, 1461720, 1476259, 1509175, 1509181, 1509182, 1535940, 1578041, 1578155, 1604047, 1604766, 1615188, 1969579, 2030361, 2186983, 2186984, 2191027, 2227069, 2342392, 2514733, 2515122, 2515192, 2515198, 2515203, 2516838, 2516839, 2775528], \n",
    "#  'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
    "\n",
    "def categoryCount():\n",
    "    from collections import defaultdict\n",
    "    import gzip\n",
    "    import json\n",
    "    import zipfile\n",
    "        \n",
    "    counts = defaultdict(int)\n",
    "    examples = 0\n",
    "    \n",
    "    with zipfile.ZipFile('Amazon-3M.raw.zip') as fzip:\n",
    "        with fzip.open('Amazon-3M.raw/trn.json.gz') as fbin:\n",
    "            with gzip.open(fbin) as f:\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    examples += 1\n",
    "                    \n",
    "                    for label in obj['target_ind']:\n",
    "                        counts[label] += 1\n",
    "\n",
    "    indices = { v: n for n, v in enumerate(counts) }\n",
    "            \n",
    "    return counts, examples, indices\n",
    "\n",
    "def embedData():\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import gzip\n",
    "    import json\n",
    "    import zipfile\n",
    "    \n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    batchsize = 20\n",
    "    \n",
    "    with zipfile.ZipFile('Amazon-3M.raw.zip') as fzip:\n",
    "        with fzip.open('Amazon-3M.raw/trn.json.gz') as fbin:\n",
    "            with gzip.open(fbin) as f:\n",
    "                batchencode, batchlabels = [], []\n",
    "\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    batchencode.append(obj['title'])\n",
    "                    batchencode.append(obj['content'])\n",
    "                    batchlabels.append(obj['target_ind'])\n",
    "                \n",
    "                    if len(batchencode) >= batchsize:\n",
    "                        embed = model.encode(batchencode)\n",
    "                    \n",
    "                        for n, labels in enumerate(batchlabels):\n",
    "                            embtitle, embcontent = embed[2*n], embed[2*n+1]\n",
    "                            yield { 'title': embtitle, \n",
    "                                    'content': embcontent, \n",
    "                                    'labels': labels }\n",
    "                            batchencode, batchlabels = [], []\n",
    "                                         \n",
    "    if len(batchencode):\n",
    "        embed = model.encode(batchencode)\n",
    "                    \n",
    "        for n, labels in enumerate(batchlabels):\n",
    "            embtitle, embcontent = embed[2*n], embed[2*n+1]\n",
    "            yield { 'title': embtitle, \n",
    "                    'content': embcontent, \n",
    "                    'labels': labels }\n",
    "            batchencode, batchlabels = [], []\n",
    "            \n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        from tqdm.notebook import tqdm\n",
    "        \n",
    "        _, examples, self.indices = categoryCount()\n",
    "        \n",
    "        Xs = []\n",
    "        ys = []\n",
    "        for n, what in tqdm(enumerate(embedData()), total=examples):\n",
    "            title = torch.tensor(what['title'])\n",
    "            content = torch.tensor(what['content'])\n",
    "            Xs.append(torch.cat((title, content)).unsqueeze(0))\n",
    "            thisy = set(self.indices[label] for label in what['labels'])\n",
    "            ys.append(thisy)\n",
    "\n",
    "        self.Xs = torch.cat(Xs, dim=0)\n",
    "        self.ys = ys\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.Xs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ys = torch.zeros(len(self.indices)).float()\n",
    "        for l in self.ys[index]:\n",
    "            ys[l] = 1.0\n",
    "        return self.Xs[index], ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5dc2b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_answer': 1889237,\n",
       " 'best_constant_average_bceloss': 0.0001429955723006122,\n",
       " 'best_constant_average_accuracy': 0.00699342627244093}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best constant predictor\n",
    "# if you don't beat this, you have a problem\n",
    "\n",
    "def bestconstant():\n",
    "    from math import fsum\n",
    "    \n",
    "    counts, examples, _ = categoryCount()\n",
    "    log_loss = torch.nn.BCELoss()\n",
    "    sumloss = EasyAcc()\n",
    "    \n",
    "    positive = torch.Tensor([1])\n",
    "    negative = torch.Tensor([0])\n",
    "    \n",
    "    for m, k in enumerate(counts.keys()):\n",
    "        n = counts[k]\n",
    "        predict = torch.Tensor([ n / examples ])\n",
    "        sumloss += n * log_loss(predict, positive).item()\n",
    "        sumloss += (examples - n) * log_loss(predict, negative).item()\n",
    "        \n",
    "    denom = examples * len(counts.keys())\n",
    "            \n",
    "    return { 'best_constant_answer': max((v, k) for k, v in counts.items())[1], \n",
    "             'best_constant_average_bceloss': sumloss.sum / denom,\n",
    "             'best_constant_average_accuracy': max(v for v in counts.values()) / examples }            \n",
    "\n",
    "bestconstant()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77462939",
   "metadata": {},
   "source": [
    "# This takes time run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec63e1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e59a95886204497b340619a68b8058e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def makeMyDataset():\n",
    "    import gzip\n",
    "    \n",
    "    foo = MyDataset()\n",
    "    with gzip.open(f'amazon3m.pickle.gz', 'wb') as handle:\n",
    "        import pickle\n",
    "        pickle.dump(foo, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "makeMyDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285f6e3",
   "metadata": {},
   "source": [
    "# Load Cached Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c23449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMyDataset():\n",
    "    import gzip\n",
    "    \n",
    "    with gzip.open(f'amazon3m.pickle.gz', 'rb') as handle:\n",
    "        import pickle\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e2423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, d, device):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.zeros(d, d, device=device))\n",
    "        self.afunc = torch.nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X + 0.001 * self.afunc(torch.matmul(X, self.W))\n",
    "\n",
    "class BilinearResidual(torch.nn.Module):\n",
    "    def __init__(self, dobs, daction, device, depth):\n",
    "        super(BilinearResidual, self).__init__()\n",
    "        \n",
    "        self.block = torch.nn.Sequential(*[ResidualBlock(dobs, device) for _ in range(depth) ])\n",
    "        self.W = torch.nn.Parameter(torch.zeros(dobs, daction-1, device=device))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, device=device))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, Xs, Zs):\n",
    "        return torch.matmul(torch.matmul(self.block(Xs), self.W), Zs[:,:-1].T) + Zs[:,-1] + self.b\n",
    "        \n",
    "    def preq1(self, logits):\n",
    "        return self.sigmoid(logits)\n",
    "\n",
    "class RankOneDetset(object):\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.N, self.K, self.D = actions.shape\n",
    "        self.device = actions.device\n",
    "        \n",
    "        self.batcheye = torch.eye(self.D, device=self.device).unsqueeze(0).expand(self.N, -1, -1)\n",
    "        self.S = self.batcheye.clone()\n",
    "        self.Sinv = self.batcheye.clone()\n",
    "        self.logdetfac = torch.zeros(self.N, device=self.device)\n",
    "        \n",
    "    def computePhi(self, i): \n",
    "        # Sprime_a <- replace column i of S with action a where det(S)=1\n",
    "        # Sprime_a = S + (a - S_i) e_i^\\top = S + u v^\\top\n",
    "        # det(Sprime_a) = det(S) (1 + e_i^\\top S^{-1} (a - S_i))\n",
    "        #               = (1 - (S^{-T} e_i)^\\top S_i) + (S^{-T} e_i)^\\top a\n",
    "        #               = 0 + \\phi^\\top a\n",
    "        \n",
    "        #Sinvtopei = torch.linalg.solve(torch.transpose(self.S, 1, 2), self.batcheye[:,:,i])\n",
    "        Sinvtopei = self.Sinv[:, i, :]\n",
    "        return Sinvtopei, self.logdetfac\n",
    "    \n",
    "    def updateCoord(self, i, fstar, astar):\n",
    "        Y = torch.gather(input=self.actions, \n",
    "                         dim=1, \n",
    "                         index=astar.reshape(self.N, 1, 1).expand(self.N, 1, self.D)\n",
    "                        ).squeeze(1)\n",
    "        Y /= torch.exp(self.logdetfac).reshape(self.N, 1)\n",
    "\n",
    "        # replace column i of S with y\n",
    "        # -----------------------------\n",
    "        # Sprime = S + (y - S_i) e_i^\\top = S + u v^\\top\n",
    "        # Sprime^{-1} = S^{-1} - 1/(1 + v^\\top S^{-1} u) (S^{-1} u) (v^\\top S^{-1})^\\top\n",
    "        \n",
    "        u = Y - self.S[:, :, i]\n",
    "        Sinvu = torch.bmm(self.Sinv, u.unsqueeze(2)).squeeze(2)\n",
    "        vtopSinv = self.Sinv[:, i, :]\n",
    "        vtopSinvu = Sinvu[:, i].unsqueeze(1).unsqueeze(2)\n",
    "        self.Sinv -= (1 / (1 + vtopSinvu)) * torch.bmm(Sinvu.unsqueeze(2), vtopSinv.unsqueeze(1))\n",
    "        \n",
    "        self.S[:,:,i] = Y\n",
    "        thislogdet = 1/self.D * (torch.log(fstar) - self.logdetfac)\n",
    "        scale = torch.exp(thislogdet).reshape(self.N, 1, 1)\n",
    "        self.S /= scale\n",
    "        self.Sinv *= scale\n",
    "        self.logdetfac += thislogdet\n",
    "    \n",
    "class SpannerEG(torch.nn.Module):\n",
    "    def __init__(self, actions, epsilon, tzero):\n",
    "        super(SpannerEG, self).__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.tzero = tzero\n",
    "        self.t = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batchactions = actions.unsqueeze(0)\n",
    "            self.spanner = self._make_spanner(batchactions)\n",
    "            \n",
    "    def _make_spanner(self, actions):\n",
    "        from math import log\n",
    "\n",
    "        # Algorithm 4 Approximate Barycentric Identification (Awerbuch and Kleinberg, 2008)\n",
    "        C = 2\n",
    "        \n",
    "        N, K, D = actions.shape\n",
    "        device = actions.device\n",
    "        #detset = NaiveDetset(actions)\n",
    "        detset = RankOneDetset(actions)\n",
    "        design = torch.zeros(N, D, device=device).long()\n",
    "                \n",
    "        for i in range(D):\n",
    "            psi, _ = detset.computePhi(i)\n",
    "            dets = torch.abs(torch.bmm(actions, psi.unsqueeze(2))).squeeze(2) \n",
    "            fstar, astar = torch.max(dets, dim=1)\n",
    "            design[:, i] = astar\n",
    "            detset.updateCoord(i, fstar, astar)\n",
    "                        \n",
    "        for _ in range(int(D * log(D))):\n",
    "            replaced = False\n",
    "            for i in range(D):\n",
    "                psi, logdetfac = detset.computePhi(i)\n",
    "                dets = torch.abs(torch.bmm(actions, psi.unsqueeze(2))).squeeze(2)\n",
    "                fstar, astar = torch.max(dets, dim=1)\n",
    "                                \n",
    "                if torch.any(fstar >= C * torch.exp(logdetfac)):\n",
    "                    design[:, i] = astar\n",
    "                    detset.updateCoord(i, fstar, astar)\n",
    "                    replaced = True\n",
    "                    break\n",
    "                    \n",
    "            if not replaced:\n",
    "                break\n",
    "                \n",
    "        return design\n",
    "\n",
    "    def sample(self, fhat):\n",
    "        epsilon = self.epsilon * pow(self.tzero / (self.t + self.tzero), 1/3)\n",
    "        self.t += 1\n",
    "        \n",
    "        exploit = torch.argmax(fhat, dim=1, keepdim=True)\n",
    "        exploreindex = torch.randint(low=0, high=self.spanner.shape[1], size=(fhat.shape[0], 1), device=fhat.device)\n",
    "        explore = torch.gather(input=self.spanner[0,:].expand(fhat.shape[0], -1), dim=1, index=exploreindex)\n",
    "        shouldexplore = (torch.rand(size=(fhat.shape[0], 1), device=fhat.device) < epsilon).long()\n",
    "        sample = shouldexplore * (explore - exploit) + exploit\n",
    "        return sample.squeeze(1)\n",
    "\n",
    "class Embedding(object):\n",
    "    def __init__(self, seed, naction):\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.cooc = defaultdict(lambda: defaultdict(int))\n",
    "        self.counts = defaultdict(int)\n",
    "        self.examples = 0\n",
    "        self.naction = naction\n",
    "    \n",
    "    def consume(self, ys):        \n",
    "        self.examples += ys.shape[0]\n",
    "        for row in range(ys.shape[0]):\n",
    "            nonzeros = [ v.item() for v in torch.nonzero(ys[row]) ]\n",
    "            for a in nonzeros:\n",
    "                assert 0 <= a < self.naction, a\n",
    "                self.counts[a] += 1\n",
    "                for b in nonzeros:\n",
    "                    assert 0 <= b < self.naction, b\n",
    "                    self.cooc[a][b] += 1\n",
    "            \n",
    "    def fit(self, rank):\n",
    "        from math import log, log1p, sqrt\n",
    "        \n",
    "        remap = {}\n",
    "        \n",
    "        # Hellinger PCA\n",
    "        row_indices, col_indices, values = [], [], []\n",
    "        for a, na in self.counts.items():\n",
    "            if a not in remap:\n",
    "                remap[a] = len(remap)\n",
    "                \n",
    "            for b, cooc_ab in self.cooc[a].items():\n",
    "                if b not in remap:\n",
    "                    remap[b] = len(remap)\n",
    "                \n",
    "                row_indices.append(remap[a])\n",
    "                col_indices.append(remap[b])\n",
    "                values.append(sqrt(cooc_ab / na))\n",
    "                \n",
    "        # throws \"not implemented error\" ... #sadlife\n",
    "        #\n",
    "        # coo = torch.sparse_coo_tensor([ row_indices, col_indices ], values)\n",
    "        # csr = coo.to_sparse_csr()\n",
    "        # return torch.svd_lowrank(csr, q=d+6, niter=2, M=None), indices\n",
    "\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        from scipy.sparse import coo_matrix\n",
    "\n",
    "        coo = coo_matrix( ( values, ( row_indices, col_indices ) ), \n",
    "                          shape = (len(remap), len(remap)) )\n",
    "        csr = coo.tocsr()\n",
    "\n",
    "        svd = TruncatedSVD(n_components=rank, \n",
    "                           algorithm='randomized',\n",
    "                           n_iter=2,\n",
    "                           random_state=self.seed)\n",
    "        svd.fit(csr)\n",
    "        Z = sqrt(self.examples) * torch.tensor(svd.transform(csr))\n",
    "        \n",
    "        assert self.examples > 0\n",
    "        defaultphat = 1 / self.examples\n",
    "        bias = torch.ones(Z.shape[0]) * (log(defaultphat) - log1p(-defaultphat))\n",
    "        for a, na in self.counts.items():\n",
    "            assert 0 <= a < self.naction\n",
    "            assert 0 < na < self.examples, (a, na, self.examples)\n",
    "            phat = na / self.examples\n",
    "            bias[remap[a]] = log(phat) - log1p(-phat)\n",
    "            \n",
    "        Z = torch.cat((Z, bias.unsqueeze(1)), dim=1)\n",
    "        inverseremap = { v: k for k, v in remap.items() }\n",
    "        remapTensor = torch.LongTensor([ inverseremap[n] for n in range(len(remap)) ]).unsqueeze(0)\n",
    "        \n",
    "        return Z, remapTensor\n",
    "    \n",
    "def embed(dataset, batch_size, pretrain, rank, seed):\n",
    "    from tqdm.notebook import tqdm\n",
    "    from math import sqrt\n",
    "    import time\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    splitseed = seed+1\n",
    "        \n",
    "    predata, _ = torch.utils.data.random_split(dataset,\n",
    "                                               lengths=[ pretrain, len(dataset) - pretrain ],\n",
    "                                               generator=torch.Generator().manual_seed(splitseed))\n",
    "    generator = torch.utils.data.DataLoader(predata, \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=True)\n",
    "    embedding = Embedding(seed=seed+2, naction=len(dataset.indices))\n",
    "    \n",
    "    print('embed', flush=True)\n",
    "    for bno, (Xs, ys) in tqdm(enumerate(generator), total=(pretrain//batch_size)):\n",
    "        embedding.consume(ys)\n",
    "        \n",
    "    print('fit', flush=True)\n",
    "    Z, remap = embedding.fit(rank)\n",
    "    print('done', flush=True)\n",
    "    \n",
    "    return Z.float(), remap, splitseed\n",
    "    \n",
    "def presup(dataset, actions, initlr, tzero, batch_size, depth, pretrain, cuda, seed):\n",
    "    from math import sqrt\n",
    "    import time\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    Zs, remap, splitseed = actions\n",
    "    \n",
    "    if cuda:\n",
    "        Zs = Zs.cuda()\n",
    "        remap = remap.cuda()\n",
    "        \n",
    "    predata, _ = torch.utils.data.random_split(dataset,\n",
    "                                               lengths=[ pretrain, len(dataset) - pretrain ],\n",
    "                                               generator=torch.Generator().manual_seed(splitseed))\n",
    "    generator = torch.utils.data.DataLoader(predata, \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=True)\n",
    "    \n",
    "    print('{:<5s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}'.format('n', 'loss', 'since last', 'acc', 'acc since last', 'dt (sec)'), flush=True)\n",
    "    avloss, acc, sincelast, accsincelast = EasyAcc(), EasyAcc(), EasyAcc(), EasyAcc()\n",
    "\n",
    "    model = None\n",
    "    log_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for bno, (Xs, preys) in enumerate(generator):\n",
    "        Xs, preys = Xs.to(Zs.device), preys.to(Zs.device)\n",
    "\n",
    "        if model is None:\n",
    "            model = BilinearResidual(dobs=Xs.shape[1], daction=Zs.shape[1], device=Zs.device, depth=depth)\n",
    "            opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: sqrt(tzero) / sqrt(tzero + t))\n",
    "            start = time.time()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            ys = torch.gather(input=preys, dim=1, index=remap.expand(preys.shape[0], -1))        \n",
    "\n",
    "        opt.zero_grad()\n",
    "        score = model.forward(0.0001 * Xs, Zs)\n",
    "        loss = log_loss(score, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(score, dim=1)\n",
    "            ypred = torch.gather(input=ys, dim=1, index=pred.unsqueeze(1))\n",
    "            acc += torch.mean(ypred).float()\n",
    "            accsincelast += torch.mean(ypred).float()\n",
    "            avloss += loss\n",
    "            sincelast += loss\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            print('{:<5d}\\t{:<8.5g}\\t{:<8.5g}\\t{:<8.5g}\\t{:<8.5g}\\t{:<8.5g}'.format(avloss.n, avloss.mean(), sincelast.mean(), \n",
    "                                                                                    acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "                  flush=True)\n",
    "            sincelast, accsincelast = EasyAcc(), EasyAcc()\n",
    "\n",
    "    print('{:<5d}\\t{:<8.5g}\\t{:<8.5g}\\t{:<8.5g}\\t{:<8.5g}\\t{:<8.5g}'.format(avloss.n, avloss.mean(), sincelast.mean(), \n",
    "                                                                            acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "          flush=True)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def train(dataset, model, actions, initlr, tzero, epsilon, epsilontzero, batch_size, pretrain, cuda, seed):\n",
    "    from math import sqrt\n",
    "    import time\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    Zs, remap, splitseed = actions\n",
    "    \n",
    "    if cuda:\n",
    "        Zs = Zs.cuda()\n",
    "        remap = remap.cuda()\n",
    "    \n",
    "    _, traindata = torch.utils.data.random_split(dataset,\n",
    "                                                 lengths=[ pretrain, len(dataset) - pretrain ],\n",
    "                                                 generator=torch.Generator().manual_seed(splitseed))\n",
    "    generator = torch.utils.data.DataLoader(traindata, \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=True)\n",
    "    \n",
    "    log_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    sampler = SpannerEG(actions=Zs, epsilon=epsilon, tzero=epsilontzero)\n",
    "    opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: sqrt(tzero) / sqrt(tzero + t))\n",
    "    start = time.time()\n",
    "        \n",
    "    print('{:<5s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}\\t{:<10s}'.format(\n",
    "            'n', 'loss', 'since last', 'acc', 'since last', 'reward', 'since last', 'dt (sec)'), \n",
    "          flush=True)\n",
    "    avloss, sincelast, acc, accsincelast, avreward, rewardsincelast = [ EasyAcc() for _ in range(6) ]\n",
    "    \n",
    "    for bno, (Xs, preys) in enumerate(generator):\n",
    "        Xs, preys = Xs.to(Zs.device), preys.to(Zs.device)\n",
    "              \n",
    "        with torch.no_grad():\n",
    "            ys = torch.gather(input=preys, dim=1, index=remap.expand(preys.shape[0], -1))        \n",
    "\n",
    "        opt.zero_grad()\n",
    "        logit = model.forward(0.0001 * Xs, Zs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample = sampler.sample(logit)\n",
    "            reward = torch.gather(input=ys, dim=1, index=sample.unsqueeze(1)).float()\n",
    "            \n",
    "        samplelogit = torch.gather(input=logit, index=sample.unsqueeze(1), dim=1)\n",
    "        loss = log_loss(samplelogit, reward)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(logit, dim=1)\n",
    "            ypred = torch.gather(input=ys, dim=1, index=pred.unsqueeze(1))\n",
    "            acc += torch.mean(ypred).float()\n",
    "            accsincelast += torch.mean(ypred).float()\n",
    "            avloss += loss\n",
    "            sincelast += loss\n",
    "            avreward += torch.mean(reward)\n",
    "            rewardsincelast += torch.mean(reward)\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            now = time.time()\n",
    "            print('{:<5d}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}'.format(\n",
    "                    avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), \n",
    "                    accsincelast.mean(), avreward.mean(), rewardsincelast.mean(),\n",
    "                    now - start),\n",
    "                  flush=True)\n",
    "            sincelast, accsincelast, rewardsincelast = [ EasyAcc() for _ in range(3) ]\n",
    "\n",
    "    now = time.time()\n",
    "    print('{:<5d}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}\\t{:<10.5f}'.format(\n",
    "            avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), \n",
    "            accsincelast.mean(), avreward.mean(), rewardsincelast.mean(),\n",
    "            now - start),\n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe35cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = loadMyDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb47b2",
   "metadata": {},
   "source": [
    "Here we use a supervised pre-training step inspired by [Sen et. al.](https://arxiv.org/abs/2102.07800), which allows us to estimate an action embedding using Hellinger PCA.  This is slow-ish (a few minutes) and a memory hog (dozens of gigabytes b/c I'm lazy and didn't implement this out-of-core) so we do it once and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d19f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEmbedding(mydata, rank, seed=2112):\n",
    "    import gzip\n",
    "    \n",
    "    embeds = embed(mydata, batch_size=32, pretrain=50000, rank=rank, seed=seed)\n",
    "    with gzip.open(f'amazon3m.embeds.{rank}.pickle.gz', 'wb') as handle:\n",
    "        import pickle\n",
    "        pickle.dump(embeds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9825c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbedding(rank):\n",
    "    import gzip\n",
    "    \n",
    "    with gzip.open(f'amazon3m.embeds.{rank}.pickle.gz', 'rb') as handle:\n",
    "        import pickle\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60de4f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank = 1600\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t10.766  \n",
      "2    \t0.00045801\t0.00046822\t0.015625\t0.03125 \t12.306  \n",
      "3    \t0.00045539\t0.00045014\t0.010417\t0       \t13.754  \n",
      "5    \t0.00041916\t0.00036482\t0.00625 \t0       \t16.683  \n",
      "9    \t0.00041313\t0.00040559\t0.0034722\t0       \t22.533  \n",
      "17   \t0.00043714\t0.00046416\t0.0036765\t0.0039062\t34.127  \n",
      "33   \t0.00045017\t0.00046401\t0.0056818\t0.0078125\t57.366  \n",
      "65   \t0.00045066\t0.00045117\t0.0086538\t0.011719\t104.01  \n",
      "129  \t0.00045077\t0.00045087\t0.016473\t0.024414\t197.07  \n",
      "257  \t0.00043483\t0.00041876\t0.026751\t0.037109\t380.95  \n",
      "513  \t0.00041991\t0.00040493\t0.044042\t0.061401\t747.33  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-23139aa59525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'rank = {rank}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     doit(rank, seed=4545, \n\u001b[0m\u001b[1;32m     18\u001b[0m          \u001b[0minitlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m          \u001b[0mbanditinitlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandittzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-23139aa59525>\u001b[0m in \u001b[0;36mdoit\u001b[0;34m(rank, initlr, tzero, banditinitlr, bandittzero, epsilon, epsilontzero, seed, prebs, banditbs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrain'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     model = presup(mydata, actions=embeds, depth=depth,\n\u001b[0m\u001b[1;32m      7\u001b[0m                    \u001b[0minitlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtzero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    batch_size=prebs, pretrain=pretrain, cuda=cuda, seed=seed)\n",
      "\u001b[0;32m<ipython-input-2-c9a178a7bf12>\u001b[0m in \u001b[0;36mpresup\u001b[0;34m(dataset, actions, initlr, tzero, batch_size, depth, pretrain, cuda, seed)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def doit(rank, *, initlr, tzero, banditinitlr, bandittzero, epsilon, epsilontzero, seed, prebs, banditbs):\n",
    "    pretrain, depth, cuda = 50000, 2, False\n",
    "    \n",
    "    embeds = loadEmbedding(rank=rank)\n",
    "    print('pretrain')\n",
    "    model = presup(mydata, actions=embeds, depth=depth,\n",
    "                   initlr=initlr, tzero=tzero, \n",
    "                   batch_size=prebs, pretrain=pretrain, cuda=cuda, seed=seed)\n",
    "    print('train')\n",
    "    train(mydata, model=model, actions=embeds, \n",
    "          initlr=banditinitlr, tzero=bandittzero,\n",
    "          epsilon=epsilon, epsilontzero=epsilontzero, \n",
    "          batch_size=banditbs, pretrain=pretrain, cuda=cuda, seed=seed)\n",
    "    \n",
    "for rank in (1600,):\n",
    "    print(f'rank = {rank}')\n",
    "    doit(rank, seed=4545, \n",
    "         initlr=1/80, tzero=100, \n",
    "         banditinitlr=1/640, bandittzero=10, \n",
    "         epsilon=1/10, epsilontzero=10,\n",
    "         prebs=32, banditbs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2556266b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank = 800\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t6.0833  \n",
      "2    \t0.00045795\t0.0004681\t0.015625\t0.03125 \t7.3707  \n",
      "3    \t0.00045246\t0.00044148\t0.010417\t0       \t8.7971  \n",
      "5    \t0.0004153\t0.00035956\t0.00625 \t0       \t11.476  \n",
      "9    \t0.00040865\t0.00040034\t0.0034722\t0       \t16.863  \n",
      "17   \t0.00043364\t0.00046175\t0.0018382\t0       \t27.55   \n",
      "33   \t0.00044654\t0.00046026\t0.0047348\t0.0078125\t48.681  \n",
      "65   \t0.00044619\t0.00044583\t0.0072115\t0.0097656\t92.33   \n",
      "129  \t0.00044572\t0.00044523\t0.016715\t0.026367\t178.91  \n",
      "257  \t0.00043035\t0.00041486\t0.027602\t0.038574\t353.86  \n",
      "513  \t0.00041687\t0.00040335\t0.045565\t0.063599\t703.79  \n",
      "1025 \t0.00040011\t0.00038331\t0.060793\t0.07605 \t1412.7  \n",
      "1563 \t0.00039106\t0.00037381\t0.069438\t0.085908\t2173.5  \n",
      "train\n",
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.28190   \t0.28190   \t0.10156   \t0.10156   \t0.08984   \t0.08984   \t7.07186   \n",
      "2    \t0.30301   \t0.32412   \t0.10938   \t0.11719   \t0.09766   \t0.10547   \t31.62720  \n",
      "3    \t0.29439   \t0.27717   \t0.09245   \t0.05859   \t0.08073   \t0.04688   \t38.36811  \n",
      "5    \t0.29774   \t0.30276   \t0.09141   \t0.08984   \t0.08203   \t0.08398   \t51.91093  \n",
      "9    \t0.30892   \t0.32290   \t0.09592   \t0.10156   \t0.08724   \t0.09375   \t78.86724  \n",
      "17   \t0.29369   \t0.27656   \t0.08869   \t0.08057   \t0.08088   \t0.07373   \t131.62386 \n",
      "33   \t0.29113   \t0.28841   \t0.08783   \t0.08691   \t0.08215   \t0.08350   \t234.60761 \n",
      "65   \t0.28819   \t0.28515   \t0.09020   \t0.09265   \t0.08480   \t0.08752   \t446.02639 \n",
      "129  \t0.28926   \t0.29036   \t0.09411   \t0.09808   \t0.08918   \t0.09363   \t865.70130 \n",
      "257  \t0.28336   \t0.27742   \t0.09442   \t0.09473   \t0.08990   \t0.09064   \t1709.04753\n",
      "513  \t0.28423   \t0.28511   \t0.09526   \t0.09610   \t0.09170   \t0.09351   \t3425.10522\n",
      "1025 \t0.28544   \t0.28666   \t0.09749   \t0.09973   \t0.09449   \t0.09727   \t6876.13181\n",
      "2049 \t0.28496   \t0.28447   \t0.09883   \t0.10017   \t0.09641   \t0.09833   \t13965.61864\n",
      "4097 \t0.28407   \t0.28319   \t0.10047   \t0.10210   \t0.09849   \t0.10058   \t28106.74236\n",
      "6516 \t0.28419   \t0.28440   \t0.10179   \t0.10404   \t0.10009   \t0.10280   \t44615.77718\n",
      "rank = 400\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t0.85143 \n",
      "2    \t0.00045839\t0.00046898\t0.015625\t0.03125 \t1.8037  \n",
      "3    \t0.00045111\t0.00043656\t0.010417\t0       \t2.79    \n",
      "5    \t0.00041331\t0.00035659\t0.00625 \t0       \t4.7523  \n",
      "9    \t0.0004067\t0.00039844\t0.0069444\t0.0078125\t8.6288  \n",
      "17   \t0.00043095\t0.00045823\t0.0055147\t0.0039062\t16.76   \n",
      "33   \t0.00044363\t0.0004571\t0.0047348\t0.0039062\t32.256  \n",
      "65   \t0.00044277\t0.00044188\t0.0072115\t0.0097656\t63.448  \n",
      "129  \t0.00044312\t0.00044348\t0.016957\t0.026855\t125.72  \n",
      "257  \t0.00042894\t0.00041466\t0.029426\t0.041992\t249.91  \n",
      "513  \t0.00041767\t0.00040635\t0.047454\t0.065552\t500.15  \n",
      "1025 \t0.00040363\t0.00038956\t0.059787\t0.072144\t998.1   \n",
      "1563 \t0.00039645\t0.00038276\t0.067638\t0.082598\t1520.6  \n",
      "train\n",
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.27435   \t0.27435   \t0.08203   \t0.08203   \t0.07031   \t0.07031   \t5.49232   \n",
      "2    \t0.28301   \t0.29166   \t0.08984   \t0.09766   \t0.08008   \t0.08984   \t11.20474  \n",
      "3    \t0.28023   \t0.27467   \t0.08073   \t0.06250   \t0.07161   \t0.05469   \t16.79142  \n",
      "5    \t0.28781   \t0.29919   \t0.08359   \t0.08789   \t0.07656   \t0.08398   \t28.46029  \n",
      "9    \t0.30291   \t0.32177   \t0.09028   \t0.09863   \t0.08333   \t0.09180   \t51.18733  \n",
      "17   \t0.28041   \t0.25511   \t0.08111   \t0.07080   \t0.07491   \t0.06543   \t95.71794  \n",
      "33   \t0.26936   \t0.25761   \t0.07955   \t0.07788   \t0.07446   \t0.07397   \t186.59554 \n",
      "65   \t0.26089   \t0.25217   \t0.08005   \t0.08057   \t0.07506   \t0.07568   \t368.02979 \n",
      "129  \t0.26409   \t0.26733   \t0.08427   \t0.08856   \t0.07982   \t0.08466   \t732.49831 \n",
      "257  \t0.26076   \t0.25741   \t0.08466   \t0.08505   \t0.08068   \t0.08154   \t1454.65991\n",
      "513  \t0.25846   \t0.25615   \t0.08474   \t0.08482   \t0.08148   \t0.08229   \t2903.48411\n",
      "1025 \t0.26177   \t0.26508   \t0.08722   \t0.08971   \t0.08457   \t0.08766   \t5806.18433\n",
      "2049 \t0.26304   \t0.26432   \t0.08857   \t0.08993   \t0.08643   \t0.08830   \t11518.89463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6c6f443dfc5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'rank = {rank}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     doit(rank, seed=4545, \n\u001b[0m\u001b[1;32m     18\u001b[0m          \u001b[0minitlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m          \u001b[0mbanditinitlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandittzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6c6f443dfc5e>\u001b[0m in \u001b[0;36mdoit\u001b[0;34m(rank, initlr, tzero, banditinitlr, bandittzero, epsilon, epsilontzero, seed, prebs, banditbs)\u001b[0m\n\u001b[1;32m      8\u001b[0m                    batch_size=prebs, pretrain=pretrain, cuda=cuda, seed=seed)\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train(mydata, model=model, actions=embeds, \n\u001b[0m\u001b[1;32m     11\u001b[0m           \u001b[0minitlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbanditinitlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandittzero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilontzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilontzero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c9a178a7bf12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, actions, initlr, tzero, epsilon, epsilontzero, batch_size, pretrain, cuda, seed)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def doit(rank, *, initlr, tzero, banditinitlr, bandittzero, epsilon, epsilontzero, seed, prebs, banditbs):\n",
    "    pretrain, depth, cuda = 50000, 2, False\n",
    "    \n",
    "    embeds = loadEmbedding(rank=rank)\n",
    "    print('pretrain')\n",
    "    model = presup(mydata, actions=embeds, depth=depth,\n",
    "                   initlr=initlr, tzero=tzero, \n",
    "                   batch_size=prebs, pretrain=pretrain, cuda=cuda, seed=seed)\n",
    "    print('train')\n",
    "    train(mydata, model=model, actions=embeds, \n",
    "          initlr=banditinitlr, tzero=bandittzero,\n",
    "          epsilon=epsilon, epsilontzero=epsilontzero, \n",
    "          batch_size=banditbs, pretrain=pretrain, cuda=cuda, seed=seed)\n",
    "    \n",
    "for rank in (800, 400, 200, 100, 50,):\n",
    "    print(f'rank = {rank}')\n",
    "    doit(rank, seed=4545, \n",
    "         initlr=1/80, tzero=100, \n",
    "         banditinitlr=1/640, bandittzero=10, \n",
    "         epsilon=1/10, epsilontzero=10,\n",
    "         prebs=32, banditbs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9ce4cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank = 50\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t0.5683  \n",
      "2    \t0.0004591\t0.0004704\t0.015625\t0.03125 \t1.2625  \n",
      "3    \t0.00045139\t0.00043596\t0.010417\t0       \t1.9251  \n",
      "5    \t0.00041331\t0.00035619\t0.00625 \t0       \t3.2894  \n",
      "9    \t0.0004051\t0.00039485\t0.0069444\t0.0078125\t6.0281  \n",
      "17   \t0.0004267\t0.00045099\t0.011029\t0.015625\t11.43   \n",
      "33   \t0.0004396\t0.00045331\t0.013258\t0.015625\t22.304  \n",
      "65   \t0.00044149\t0.00044344\t0.012019\t0.010742\t44.281  \n",
      "129  \t0.00044639\t0.00045136\t0.016715\t0.021484\t87.675  \n",
      "257  \t0.00043907\t0.00043169\t0.024197\t0.031738\t174     \n",
      "513  \t0.00043631\t0.00043355\t0.032103\t0.040039\t345.23  \n",
      "1025 \t0.00043005\t0.00042378\t0.038018\t0.043945\t692.64  \n",
      "1563 \t0.00042766\t0.00042311\t0.040087\t0.044029\t1054.5  \n",
      "train\n",
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.17400   \t0.17400   \t0.05859   \t0.05859   \t0.04688   \t0.04688   \t3.82617   \n",
      "2    \t0.17429   \t0.17458   \t0.05664   \t0.05469   \t0.04492   \t0.04297   \t22.05028  \n",
      "3    \t0.17353   \t0.17200   \t0.04818   \t0.03125   \t0.03906   \t0.02734   \t25.60938  \n",
      "5    \t0.17942   \t0.18826   \t0.05234   \t0.05859   \t0.04531   \t0.05469   \t38.85601  \n",
      "9    \t0.19987   \t0.22542   \t0.05729   \t0.06348   \t0.05122   \t0.05859   \t53.37756  \n",
      "17   \t0.18052   \t0.15876   \t0.05101   \t0.04395   \t0.04619   \t0.04053   \t83.85019  \n",
      "33   \t0.17107   \t0.16103   \t0.04912   \t0.04712   \t0.04534   \t0.04443   \t144.02829 \n",
      "65   \t0.16320   \t0.15509   \t0.04730   \t0.04541   \t0.04399   \t0.04260   \t267.12486 \n",
      "129  \t0.16216   \t0.16109   \t0.04787   \t0.04846   \t0.04524   \t0.04651   \t509.72687 \n",
      "257  \t0.15925   \t0.15633   \t0.04744   \t0.04700   \t0.04531   \t0.04538   \t991.46397 \n",
      "513  \t0.15640   \t0.15353   \t0.04668   \t0.04591   \t0.04502   \t0.04472   \t1934.62101\n",
      "1025 \t0.15814   \t0.15988   \t0.04779   \t0.04891   \t0.04646   \t0.04790   \t3836.70236\n",
      "2001 \t0.15939   \t0.16071   \t0.04850   \t0.04924   \t0.04736   \t0.04830   \t7518.04913\n",
      "rank = 100\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t0.56091 \n",
      "2    \t0.00045894\t0.00047006\t0.015625\t0.03125 \t1.256   \n",
      "3    \t0.00045172\t0.0004373\t0.020833\t0.03125 \t1.9505  \n",
      "5    \t0.00041345\t0.00035603\t0.0125  \t0       \t3.2688  \n",
      "9    \t0.00040482\t0.00039404\t0.010417\t0.0078125\t5.8105  \n",
      "17   \t0.00042643\t0.00045075\t0.012868\t0.015625\t11.165  \n",
      "33   \t0.00043954\t0.00045347\t0.015152\t0.017578\t21.822  \n",
      "65   \t0.00044139\t0.00044329\t0.0125  \t0.0097656\t43.615  \n",
      "129  \t0.00044415\t0.00044696\t0.019622\t0.026855\t86.46   \n",
      "257  \t0.00043438\t0.00042453\t0.027845\t0.036133\t172.51  \n",
      "513  \t0.00042859\t0.00042278\t0.039778\t0.051758\t342.65  \n",
      "1025 \t0.00041972\t0.00041084\t0.047256\t0.054749\t680.91  \n",
      "1563 \t0.00041576\t0.00040822\t0.051484\t0.059538\t1035.8  \n",
      "train\n",
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.26307   \t0.26307   \t0.08203   \t0.08203   \t0.07422   \t0.07422   \t3.43684   \n",
      "2    \t0.24432   \t0.22557   \t0.07422   \t0.06641   \t0.06445   \t0.05469   \t7.02504   \n",
      "3    \t0.23657   \t0.22107   \t0.06120   \t0.03516   \t0.05339   \t0.03125   \t10.60244  \n",
      "5    \t0.23354   \t0.22899   \t0.06719   \t0.07617   \t0.05937   \t0.06836   \t17.68572  \n",
      "9    \t0.23915   \t0.24617   \t0.06554   \t0.06348   \t0.05946   \t0.05957   \t31.22866  \n",
      "17   \t0.21969   \t0.19780   \t0.05951   \t0.05273   \t0.05423   \t0.04834   \t59.48362  \n",
      "33   \t0.21312   \t0.20613   \t0.06013   \t0.06079   \t0.05587   \t0.05762   \t115.12256 \n",
      "65   \t0.19942   \t0.18530   \t0.05835   \t0.05652   \t0.05409   \t0.05225   \t226.54927 \n",
      "129  \t0.20162   \t0.20384   \t0.06062   \t0.06293   \t0.05696   \t0.05988   \t452.95550 \n",
      "257  \t0.19958   \t0.19753   \t0.06122   \t0.06183   \t0.05812   \t0.05930   \t900.34395 \n",
      "513  \t0.19660   \t0.19360   \t0.06059   \t0.05995   \t0.05821   \t0.05830   \t1798.46324\n",
      "1025 \t0.19830   \t0.20000   \t0.06175   \t0.06291   \t0.05987   \t0.06153   \t3590.85619\n",
      "2001 \t0.19947   \t0.20070   \t0.06253   \t0.06335   \t0.06100   \t0.06220   \t7106.63880\n",
      "rank = 200\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t0.57306 \n",
      "2    \t0.00045785\t0.00046788\t0       \t0       \t1.2634  \n",
      "3    \t0.00045151\t0.00043885\t0.010417\t0.03125 \t1.9433  \n",
      "5    \t0.00041413\t0.00035804\t0.00625 \t0       \t3.2948  \n",
      "9    \t0.00040653\t0.00039704\t0.0069444\t0.0078125\t6.0044  \n",
      "17   \t0.00042849\t0.00045319\t0.0073529\t0.0078125\t11.448  \n",
      "33   \t0.0004416\t0.00045552\t0.0094697\t0.011719\t22.363  \n",
      "65   \t0.00044098\t0.00044034\t0.010577\t0.011719\t44.793  \n",
      "129  \t0.00044197\t0.00044298\t0.018895\t0.027344\t89.136  \n",
      "257  \t0.00042988\t0.00041769\t0.02821 \t0.037598\t177.65  \n",
      "513  \t0.00042121\t0.00041251\t0.045443\t0.062744\t354.43  \n",
      "1025 \t0.00041023\t0.00039923\t0.057043\t0.068665\t711.13  \n",
      "1563 \t0.00040481\t0.0003945\t0.06254 \t0.073013\t1081.7  \n",
      "train\n",
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.27530   \t0.27530   \t0.08984   \t0.08984   \t0.08203   \t0.08203   \t3.68208   \n",
      "2    \t0.27088   \t0.26646   \t0.08594   \t0.08203   \t0.07617   \t0.07031   \t7.93635   \n",
      "3    \t0.27620   \t0.28682   \t0.07552   \t0.05469   \t0.06771   \t0.05078   \t11.83528  \n",
      "5    \t0.27714   \t0.27855   \t0.07734   \t0.08008   \t0.07109   \t0.07617   \t19.56710  \n",
      "9    \t0.28352   \t0.29149   \t0.07986   \t0.08301   \t0.07422   \t0.07812   \t35.13681  \n",
      "17   \t0.26420   \t0.24247   \t0.07514   \t0.06982   \t0.06962   \t0.06445   \t66.13244  \n",
      "33   \t0.25613   \t0.24756   \t0.07386   \t0.07251   \t0.06913   \t0.06860   \t127.35499 \n",
      "65   \t0.24444   \t0.23237   \t0.07320   \t0.07251   \t0.06833   \t0.06750   \t249.67994 \n",
      "129  \t0.24701   \t0.24963   \t0.07673   \t0.08032   \t0.07249   \t0.07672   \t500.06621 \n",
      "257  \t0.23918   \t0.23128   \t0.07592   \t0.07510   \t0.07235   \t0.07220   \t1005.68202\n",
      "513  \t0.23846   \t0.23773   \t0.07655   \t0.07718   \t0.07370   \t0.07506   \t2011.72739\n",
      "1025 \t0.23909   \t0.23972   \t0.07784   \t0.07913   \t0.07553   \t0.07735   \t3990.47546\n",
      "2001 \t0.24030   \t0.24157   \t0.07885   \t0.07992   \t0.07700   \t0.07854   \t7798.28848\n",
      "rank = 400\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t0.70159 \n",
      "2    \t0.00045838\t0.00046894\t0.015625\t0.03125 \t1.4821  \n",
      "3    \t0.00045111\t0.00043657\t0.010417\t0       \t2.2714  \n",
      "5    \t0.00041335\t0.00035671\t0.00625 \t0       \t3.8549  \n",
      "9    \t0.00040687\t0.00039878\t0.0069444\t0.0078125\t6.9557  \n",
      "17   \t0.00043089\t0.00045791\t0.0055147\t0.0039062\t13.105  \n",
      "33   \t0.00044334\t0.00045657\t0.0047348\t0.0039062\t25.536  \n",
      "65   \t0.00044258\t0.0004418\t0.0081731\t0.011719\t50.204  \n",
      "129  \t0.000443\t0.00044343\t0.0172  \t0.026367\t99.511  \n",
      "257  \t0.00042888\t0.00041466\t0.029426\t0.041748\t198.61  \n",
      "513  \t0.00041769\t0.00040646\t0.047819\t0.066284\t397     \n",
      "1025 \t0.0004038\t0.00038989\t0.060122\t0.072449\t793.53  \n",
      "1563 \t0.00039672\t0.00038323\t0.067558\t0.081726\t1209.3  \n",
      "train\n",
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.28922   \t0.28922   \t0.08203   \t0.08203   \t0.07422   \t0.07422   \t4.61940   \n",
      "2    \t0.28575   \t0.28227   \t0.08984   \t0.09766   \t0.08203   \t0.08984   \t9.60687   \n",
      "3    \t0.27926   \t0.26630   \t0.07943   \t0.05859   \t0.07161   \t0.05078   \t14.53450  \n",
      "5    \t0.29358   \t0.31505   \t0.08438   \t0.09180   \t0.07812   \t0.08789   \t23.91048  \n",
      "9    \t0.31021   \t0.33100   \t0.09115   \t0.09961   \t0.08464   \t0.09277   \t42.71232  \n",
      "17   \t0.28798   \t0.26297   \t0.08295   \t0.07373   \t0.07675   \t0.06787   \t79.24536  \n",
      "33   \t0.27275   \t0.25657   \t0.08002   \t0.07690   \t0.07493   \t0.07300   \t152.78675 \n",
      "65   \t0.26439   \t0.25578   \t0.08089   \t0.08179   \t0.07590   \t0.07690   \t298.56208 \n",
      "129  \t0.26766   \t0.27097   \t0.08548   \t0.09015   \t0.08100   \t0.08618   \t593.58675 \n",
      "257  \t0.26277   \t0.25784   \t0.08565   \t0.08582   \t0.08167   \t0.08234   \t1213.43645\n",
      "513  \t0.26075   \t0.25871   \t0.08580   \t0.08595   \t0.08248   \t0.08330   \t2401.34974\n",
      "1025 \t0.26309   \t0.26545   \t0.08788   \t0.08997   \t0.08518   \t0.08789   \t4760.66735\n",
      "2001 \t0.26377   \t0.26447   \t0.08901   \t0.09020   \t0.08681   \t0.08852   \t9214.42129\n",
      "rank = 800\n",
      "pretrain\n",
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.00044781\t0.00044781\t0       \t0       \t0.86485 \n",
      "2    \t0.00045794\t0.00046807\t0.015625\t0.03125 \t1.9161  \n",
      "3    \t0.00045246\t0.0004415\t0.010417\t0       \t2.9679  \n",
      "5    \t0.00041537\t0.00035973\t0.00625 \t0       \t5.1372  \n",
      "9    \t0.00040888\t0.00040077\t0.0034722\t0       \t9.2639  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17   \t0.00043356\t0.00046132\t0.0036765\t0.0039062\t17.46   \n",
      "33   \t0.00044616\t0.00045955\t0.0056818\t0.0078125\t33.91   \n",
      "65   \t0.00044586\t0.00044554\t0.0081731\t0.010742\t66.906  \n",
      "129  \t0.00044547\t0.00044507\t0.016715\t0.025391\t132.67  \n",
      "257  \t0.00043017\t0.00041476\t0.027359\t0.038086\t263.96  \n",
      "513  \t0.00041682\t0.00040342\t0.044834\t0.062378\t526.23  \n",
      "1025 \t0.00040027\t0.00038368\t0.060305\t0.075806\t1051.3  \n",
      "1563 \t0.00039135\t0.00037436\t0.068998\t0.08556 \t1599.5  \n",
      "train\n",
      "n    \tloss      \tsince last\tacc       \tsince last\treward    \tsince last\tdt (sec)  \n",
      "1    \t0.29762   \t0.29762   \t0.10547   \t0.10547   \t0.09375   \t0.09375   \t5.68277   \n",
      "2    \t0.31429   \t0.33096   \t0.11133   \t0.11719   \t0.09961   \t0.10547   \t16.14700  \n",
      "3    \t0.30632   \t0.29037   \t0.09245   \t0.05469   \t0.08073   \t0.04297   \t21.87645  \n",
      "5    \t0.31091   \t0.31780   \t0.09141   \t0.08984   \t0.08203   \t0.08398   \t33.55211  \n",
      "9    \t0.31583   \t0.32197   \t0.09462   \t0.09863   \t0.08594   \t0.09082   \t56.73665  \n",
      "17   \t0.29704   \t0.27591   \t0.08778   \t0.08008   \t0.07973   \t0.07275   \t102.64521 \n",
      "33   \t0.29698   \t0.29691   \t0.08866   \t0.08960   \t0.08274   \t0.08594   \t194.25353 \n",
      "65   \t0.29032   \t0.28346   \t0.09044   \t0.09229   \t0.08492   \t0.08716   \t379.04931 \n",
      "129  \t0.29068   \t0.29104   \t0.09433   \t0.09827   \t0.08930   \t0.09375   \t744.98234 \n",
      "257  \t0.28321   \t0.27568   \t0.09421   \t0.09409   \t0.08974   \t0.09018   \t1477.55806\n",
      "513  \t0.28400   \t0.28481   \t0.09543   \t0.09665   \t0.09189   \t0.09406   \t2947.97036\n",
      "1025 \t0.28547   \t0.28694   \t0.09788   \t0.10033   \t0.09488   \t0.09787   \t5927.40421\n",
      "2001 \t0.28583   \t0.28620   \t0.09938   \t0.10096   \t0.09694   \t0.09911   \t11550.82960\n"
     ]
    }
   ],
   "source": [
    "def doit(rank, *, initlr, tzero, banditinitlr, bandittzero, epsilon, epsilontzero, seed, prebs, banditbs):\n",
    "    pretrain, depth, cuda = 50000, 2, False\n",
    "    \n",
    "    embeds = loadEmbedding(rank=rank)\n",
    "    print('pretrain')\n",
    "    model = presup(mydata, actions=embeds, depth=depth,\n",
    "                   initlr=initlr, tzero=tzero, \n",
    "                   batch_size=prebs, pretrain=pretrain, cuda=cuda, seed=seed)\n",
    "    print('train')\n",
    "    train(mydata, model=model, actions=embeds, \n",
    "          initlr=banditinitlr, tzero=bandittzero,\n",
    "          epsilon=epsilon, epsilontzero=epsilontzero, \n",
    "          batch_size=banditbs, pretrain=pretrain, cuda=cuda, seed=seed)\n",
    "    \n",
    "for rank in (50, 100, 200, 400, 800,):\n",
    "    print(f'rank = {rank}')\n",
    "    doit(rank, seed=4545, \n",
    "         initlr=1/80, tzero=100, \n",
    "         banditinitlr=1/640, bandittzero=10, \n",
    "         epsilon=1/10, epsilontzero=10,\n",
    "         prebs=32, banditbs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5814b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
